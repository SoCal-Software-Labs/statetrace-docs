"use strict";(self.webpackChunkstatetrace_docs=self.webpackChunkstatetrace_docs||[]).push([[477],{10:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"build-a-postgres-proxy","metadata":{"permalink":"/blog/build-a-postgres-proxy","source":"@site/blog/2021-11-16-postgres-proxy/index.md","title":"Build a Postgres Proxy in Elixir using Pattern Matching","description":"Build a postgres proxy in Elixir","date":"2021-11-16T00:00:00.000Z","formattedDate":"November 16, 2021","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"elixir","permalink":"/blog/tags/elixir"},{"label":"proxy","permalink":"/blog/tags/proxy"}],"readingTime":8.44,"truncated":true,"authors":[{"name":"Kyle Hanson","title":"CEO of Statetrace","url":"https://github.com/hansonkd","imageURL":"https://github.com/hansonkd.png","key":"kyle-hanson"}],"nextItem":{"title":"Stream Postgres Changes into Elasticsearch with Statetrace","permalink":"/blog/stream-postgres-elastic-search"}},"content":"Want to learn how your Application talks to your Database? Build a proxy using Elixir with its powerful pattern matching and `gen_tcp` to take your database understanding to the next level. In this article we build a Postgres proxy in Elixir to show all you need is a little curiousity to master one of the most popular SQL protocols around.\\n\\n\x3c!--truncate--\x3e\\n\\n<div style={{\\"background\\": \\"url(/img/ipanema.jpg)\\", \\"backgroundSize\\": \\"150px\\", width: \\"100%\\", height: \\"200px\\", \\"marginTop\\": \\"30px\\", \\"marginBottom\\": \\"30px\\"}}>\\n\\n</div>\\n\\nBuilding a proxy can be a great way to understand a protocol. Everything you build doesn\'t need to be used in production in order for it to be valuable to your understanding of the tech. The better you understand the tech, the more powerful the tech becomes.\\n\\n\\n### Accepting the socket\\n\\nFor this proxy we will open a TCP socket in active mode. In order to understand what active mode is, it\'s helpful to know what it isn\'t. When active is false, the VM will stop reading packets from the socket until you call  `:gen_tcp.recv`. When the socket is set to active mode, the VM instead reads data as fast as possible from the socket and uses the processes mailbox as the buffer.\\n\\nBecause we will be reading data as fast as possible from both upstream and downstream we will need to split out our listener into two processes. One process will read upstream data, parse it into commands, and then forward those commands to the Postgres database. The other process will read the responses from the Postgres database and forward them directly back to the client.\\n\\nTo build this proxy, I used the postgres [reference documentation](https://www.postgresql.org/docs/9.3/protocol.html) regarding the protocol.\\n\\nThis blog post assumes you are familar with Elixir and its Application structure. If you want to learn more about TCP connections and supervision, read the [official Elixir article](https://elixir-lang.org/getting-started/mix-otp/task-and-gen-tcp.html).\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n  require Logger\\n\\n  def accept(port) do\\n    {:ok, socket} =\\n      :gen_tcp.listen(port, [:binary, active: true, reuseaddr: true, packet: 0, nodelay: true])\\n\\n    Logger.info(\\"Accepting connections on port #{port}\\")\\n    loop_acceptor(socket)\\n  end\\n\\n  defp loop_acceptor(socket) do\\n    {:ok, client} = :gen_tcp.accept(socket)\\n\\n    {:ok, pid} =\\n      Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn ->\\n        {:ok, outbound} =\\n          :gen_tcp.connect(\'localhost\', 5432, [:binary, active: true, packet: 0, nodelay: true])\\n\\n        {:ok, pid2} =\\n          Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn ->\\n            serve_downstream(client, outbound)\\n          end)\\n\\n        :ok = :gen_tcp.controlling_process(outbound, pid2)\\n\\n        serve_upstream(client, outbound, nil, true)\\n      end)\\n\\n    :ok = :gen_tcp.controlling_process(client, pid)\\n\\n    loop_acceptor(socket)\\n  end\\n\\nend\\n\\n```\\n\\nWe use `:gen_tcp.connect` to connect to the real postgres database and spin it off into a loop of its own to pipe responses back to the client.\\n\\n#### Serving upstream\\n\\nNow build the parser for the upstream connection\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n\\n  ...\\n\\n  def serve_upstream(socket, outbound, nil, is_first) do\\n    data = socket |> read_line()\\n\\n    handle_parse(socket, outbound, parse_msg(data, is_first))\\n  end\\n\\n  def serve_upstream(socket, outbound, fun, _is_first) do\\n    data = socket |> read_line()\\n\\n    r = fun.(data)\\n\\n    handle_parse(socket, outbound, r)\\n  end\\n\\n  def read_line(socket) do\\n    receive do\\n      {:tcp, ^socket, data} -> data\\n    end\\n  end\\nend\\n```\\n\\nTo parse the message is very simple. It is a very simple length-prefixed binary format with a message tag byte. The very first message of a connection is the only exception and excludes the tag. There is a chance that the current data we have is not enough to satisfy the length of the message. In this case we will use a continuation so that the next data that comes in, we can check to see if it completes the message.\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n\\n  ...\\n\\n  # On the first message don\'t extract the tag\\n  defp parse_msg(bin, true) do\\n    # Use pattern matching to extract the length\\n    <<len::unsigned-integer-32, _other_rest::binary>> = bin\\n\\n    case bin do\\n      # Pattern match to see if our binary is big enough\\n      <<msg_body::binary-size(len), final_rest::binary>> ->\\n        {:ok, {{:msgStartup, nil}, msg_body}, final_rest}\\n\\n      _ ->\\n        {:continuation,\\n         fn data ->\\n           handle_continuation(len, {:msgStartup, nil}, bin, data)\\n         end}\\n    end\\n  end\\n\\n  # Pattern match the binary to extract the tag.\\n  defp parse_msg(<<c::size(8), rest::binary>>, false) do\\n    tag = tag_to_msg_type(c)\\n\\n    # Use pattern matching to extract the length\\n    <<len::unsigned-integer-32, _other_rest::binary>> = rest\\n\\n    case rest do\\n      # Pattern match to see if our binary is big enough\\n      <<msg_body::binary-size(len), final_rest::binary>> ->\\n        {:ok, {{tag, c}, msg_body}, final_rest}\\n\\n      _ ->\\n        {:continuation,\\n         fn data ->\\n           handle_continuation(len, {tag, c}, rest, data)\\n         end}\\n    end\\n  end\\n\\n  def handle_continuation(l, tag, other, data) do\\n    new_data = other <> data\\n\\n    case new_data do\\n      <<msg_body::binary-size(l), rest::binary>> ->\\n        {:ok, {tag, msg_body}, rest}\\n\\n      _ ->\\n        {:continuation,\\n         fn data ->\\n           handle_continuation(l, tag, new_data, data)\\n         end}\\n    end\\n  end\\nend\\n```\\n\\nThe first byte in non-connection messages is a tag. We will convert this tag into an atom.\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n\\n  ...\\n\\n  defp tag_to_msg_type(val) do\\n    case val do\\n      ?1 -> :msgParseComplete\\n      ?2 -> :msgBindComplete\\n      ?3 -> :msgCloseComplete\\n      ?A -> :msgNotificationResponse\\n      ?c -> :msgCopyDone\\n      ?C -> :msgCommandComplete\\n      ?d -> :msgCopyData\\n      ?D -> :msgDataRow\\n      ?E -> :msgErrorResponse\\n      ?f -> :msgFail\\n      ?G -> :msgCopyInResponse\\n      ?H -> :msgCopyOutResponse\\n      ?I -> :msgEmptyQueryResponse\\n      ?K -> :msgBackendKeyData\\n      ?n -> :msgNoData\\n      ?N -> :msgNoticeResponse\\n      ?R -> :msgAuthentication\\n      ?s -> :msgPortalSuspended\\n      ?S -> :msgParameterStatus\\n      ?t -> :msgParameterDescription\\n      ?T -> :msgRowDescription\\n      ?p -> :msgPasswordMessage\\n      ?W -> :CopyBothResponse\\n      ?Q -> :msgQuery\\n      ?X -> :msgTerminate\\n      ?Z -> :msgReadyForQuery\\n      ?P -> :msgParse\\n      ?B -> :msgBind\\n      _ -> :msgNoTag\\n    end\\n  end\\nend\\n```\\n\\nFinally we will handle the parse result. \\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n\\n  ...\\n\\n  def handle_parse(socket, outbound, {:continuation, continuation}) do\\n    serve_upstream(socket, outbound, continuation, false)\\n  end\\n\\n  def handle_parse(socket, outbound, {:ok, {{_msgType, c}, data}, left_over}) do\\n    to_send =\\n      case c do\\n        nil -> data\\n        _ -> [c, data]\\n      end\\n\\n    :ok = :gen_tcp.send(outbound, to_send)\\n\\n    case left_over do\\n      \\"\\" -> serve_upstream(socket, outbound, nil, false)\\n      _ -> handle_parse(socket, outbound, parse_msg(left_over, false))\\n    end\\n  end\\nend\\n```\\n\\n\\n### Serving downstream\\n\\nThe downstream response is even simpler. We will not parse the message and simply forward the data directly to the socket.\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n\\n  ...\\n\\n  def serve_downstream(socket, outbound) do\\n    data = outbound |> read_line()\\n    :ok = :gen_tcp.send(socket, data)\\n\\n    serve_downstream(socket, outbound)\\n  end\\nend\\n```\\n\\n### Complete server\\n\\nThe complete server is less than 200 lines of code.\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n  require Logger\\n\\n  def accept(port) do\\n    {:ok, socket} =\\n      :gen_tcp.listen(port, [:binary, active: true, reuseaddr: true, packet: 0, nodelay: true])\\n\\n    Logger.info(\\"Accepting connections on port #{port}\\")\\n    loop_acceptor(socket)\\n  end\\n\\n  defp loop_acceptor(socket) do\\n    {:ok, client} = :gen_tcp.accept(socket)\\n\\n    {:ok, pid} =\\n      Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn ->\\n        {:ok, outbound} =\\n          :gen_tcp.connect(\'localhost\', 5432, [:binary, active: true, packet: 0, nodelay: true])\\n\\n        {:ok, pid2} =\\n          Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn ->\\n            serve_downstream(client, outbound)\\n          end)\\n\\n        :ok = :gen_tcp.controlling_process(outbound, pid2)\\n\\n        serve_upstream(client, outbound, nil, true)\\n      end)\\n\\n    :ok = :gen_tcp.controlling_process(client, pid)\\n\\n    loop_acceptor(socket)\\n  end\\n\\n  defp serve_upstream(socket, outbound, nil, is_first) do\\n    data = socket |> read_line()\\n\\n    handle_parse(socket, outbound, parse_msg(data, is_first))\\n  end\\n\\n  defp serve_upstream(socket, outbound, fun, _is_first) do\\n    data = socket |> read_line()\\n\\n    r = fun.(data)\\n\\n    handle_parse(socket, outbound, r)\\n  end\\n\\n  defp handle_parse(socket, outbound, {:continuation, continuation}) do\\n    serve_upstream(socket, outbound, continuation, false)\\n  end\\n\\n  defp handle_parse(socket, outbound, {:ok, {{_msgType, c}, data}, left_over}) do\\n    to_send =\\n      case c do\\n        nil -> data\\n        _ -> [c, data]\\n      end\\n\\n    :ok = :gen_tcp.send(outbound, to_send)\\n\\n    case left_over do\\n      \\"\\" -> serve_upstream(socket, outbound, nil, false)\\n      _ -> handle_parse(socket, outbound, parse_msg(left_over, false))\\n    end\\n  end\\n\\n  defp serve_downstream(socket, outbound) do\\n    data = outbound |> read_line()\\n    :ok = :gen_tcp.send(socket, data)\\n\\n    serve_downstream(socket, outbound)\\n  end\\n\\n  defp read_line(socket) do\\n    receive do\\n      {:tcp, ^socket, data} -> data\\n    end\\n  end\\n\\n  defp tag_to_msg_type(val) do\\n    case val do\\n      ?1 -> :msgParseComplete\\n      ?2 -> :msgBindComplete\\n      ?3 -> :msgCloseComplete\\n      ?A -> :msgNotificationResponse\\n      ?c -> :msgCopyDone\\n      ?C -> :msgCommandComplete\\n      ?d -> :msgCopyData\\n      ?D -> :msgDataRow\\n      ?E -> :msgErrorResponse\\n      ?f -> :msgFail\\n      ?G -> :msgCopyInResponse\\n      ?H -> :msgCopyOutResponse\\n      ?I -> :msgEmptyQueryResponse\\n      ?K -> :msgBackendKeyData\\n      ?n -> :msgNoData\\n      ?N -> :msgNoticeResponse\\n      ?R -> :msgAuthentication\\n      ?s -> :msgPortalSuspended\\n      ?S -> :msgParameterStatus\\n      ?t -> :msgParameterDescription\\n      ?T -> :msgRowDescription\\n      ?p -> :msgPasswordMessage\\n      ?W -> :CopyBothResponse\\n      ?Q -> :msgQuery\\n      ?X -> :msgTerminate\\n      ?Z -> :msgReadyForQuery\\n      ?P -> :msgParse\\n      ?B -> :msgBind\\n      _ -> :msgNoTag\\n    end\\n  end\\n\\n  defp parse_msg(bin, true) do\\n    <<len::unsigned-integer-32, _other_rest::binary>> = bin\\n\\n    case bin do\\n      <<msg_body::binary-size(len), final_rest::binary>> ->\\n        {:ok, {{:msgStartup, nil}, msg_body}, final_rest}\\n\\n      _ ->\\n        {:continuation,\\n         fn data ->\\n           handle_continuation(len, {:msgStartup, nil}, bin, data)\\n         end}\\n    end\\n  end\\n\\n  defp parse_msg(<<c::size(8), rest::binary>>, false) do\\n    tag = tag_to_msg_type(c)\\n\\n    <<len::unsigned-integer-32, _other_rest::binary>> = rest\\n\\n    case rest do\\n      <<msg_body::binary-size(len), final_rest::binary>> ->\\n        {:ok, {{tag, c}, msg_body}, final_rest}\\n\\n      _ ->\\n        {:continuation,\\n         fn data ->\\n           handle_continuation(len, {tag, c}, rest, data)\\n         end}\\n    end\\n  end\\n\\n  defp handle_continuation(l, tag, other, data) do\\n    new_data = other <> data\\n\\n    case new_data do\\n      <<msg_body::binary-size(l), rest::binary>> ->\\n        {:ok, {tag, msg_body}, rest}\\n\\n      _ ->\\n        {:continuation,\\n         fn data ->\\n           handle_continuation(l, tag, new_data, data)\\n         end}\\n    end\\n  end\\nend\\n```\\n\\nTo start it with your application, add it to your supervisor tree:\\n\\n```elixir\\ndefmodule Statetrace.Application do\\n  use Application\\n\\n  def start(_type, _args) do\\n\\n    children = [\\n      {Task.Supervisor, name: Statetrace.TaskSupervisor},\\n      Supervisor.child_spec({Task, fn -> Statetrace.PostgresProxy.accept(5433) end},\\n        restart: :permanent\\n      )\\n\\n    ]\\n    opts = [strategy: :one_for_one, name: Statetrace.Supervisor]\\n    Supervisor.start_link(children, opts)\\n  end\\nend\\n```\\n\\nNow you can connect to your proxy from postgres clients in every language!\\n\\nElixir:\\n\\n```elixir\\n{:ok, conn} = Postgrex.start_link(host: \\"localhost\\", port: 5433, password: \\"postgres\\", username: \\"postgres\\", database: \\"postgres\\")\\nPostgrex.query!(conn, \\"SELECT 1;\\", [])\\n```\\n\\nIn Python:\\n```python\\nconn = psycopg2.connect(\\"dbname=postgres host=localhost port=5433 user=postgres password=postgres\\")\\ncur = conn.cursor()\\ncur.execute(\\"SELECT 1;\\")\\n```\\n\\n\\n### Controlling Postgres Statements\\n\\nSo you\'re probabaly asking what can we do with this proxy? If we change our `handle_parse` function we can \\"police\\" the queries made to the upstream server. The format of the query message is simple, it is the query string you submitted with a null byte.\\n\\n```elixir\\ndefmodule Statetrace.PostgresProxy do\\n  ...\\n\\n  defp is_query_good?(q) do\\n    # In reality you will want something more secure than this\\n    String.starts_with?(q, \\"SELECT\\")\\n  end\\n\\n  defp handle_parse(socket, outbound, {:ok, {{:msgQuery, _c}, data}, _left_over} = msg) do\\n    query = String.trim_trailing(data, <<0>>)\\n    Logger.info(\\"Query: #{query}\\")\\n\\n    if is_query_good?(query) do\\n      do_handle_parse(socket, outbound, msg)\\n    else\\n      raise \\"Unauthorized Query\\"\\n    end\\n  end\\n\\n  defp handle_parse(socket, outbound, msg) do\\n    do_handle_parse(socket, outbound, msg)\\n  end\\n\\n  defp do_handle_parse(socket, outbound, {:ok, {{_msgType, c}, data}, left_over}) do\\n    to_send =\\n      case c do\\n        nil -> data\\n        _ -> [c, data]\\n      end\\n\\n    :ok = :gen_tcp.send(outbound, to_send)\\n\\n    case left_over do\\n      \\"\\" -> serve_upstream(socket, outbound, nil, false)\\n      _ -> handle_parse(socket, outbound, parse_msg(left_over, false))\\n    end\\n  end\\nend\\n```\\n\\n### Conclusion\\n\\nThere is still more to the protocol, but we have shown that with a little patience you can produce a useful proxy that understands the protocol in less than 200 lines of code. Elixir\'s pattern matching simplifies extracting information from the binary stream and `gen_tcp` makes it simple to deal with sockets.\\n\\nLike what you read? Reach out at [hello@statetrace.com](mailto:hello@statetrace.com) to learn more about what we are working on."},{"id":"stream-postgres-elastic-search","metadata":{"permalink":"/blog/stream-postgres-elastic-search","source":"@site/blog/2021-11-13-postgres-elasticsearch/index.md","title":"Stream Postgres Changes into Elasticsearch with Statetrace","description":"Stream Postgres Changes into Elasticsearch with Statetrace","date":"2021-11-13T00:00:00.000Z","formattedDate":"November 13, 2021","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"elixir","permalink":"/blog/tags/elixir"},{"label":"elasticsearch","permalink":"/blog/tags/elasticsearch"},{"label":"elastic","permalink":"/blog/tags/elastic"},{"label":"search","permalink":"/blog/tags/search"},{"label":"statetrace","permalink":"/blog/tags/statetrace"}],"readingTime":3.84,"truncated":true,"authors":[{"name":"Kyle Hanson","title":"CEO of Statetrace","url":"https://github.com/hansonkd","imageURL":"https://github.com/hansonkd.png","key":"kyle-hanson"}],"prevItem":{"title":"Build a Postgres Proxy in Elixir using Pattern Matching","permalink":"/blog/build-a-postgres-proxy"},"nextItem":{"title":"Build an Elixir Redis API that\'s 100x faster than HTTP","permalink":"/blog/redis-server"}},"content":"Need to connect Postgres to Elasticsearch? Statetrace makes it easy to stream changes from your application database into the popular search engine.\\n\\n\x3c!--truncate--\x3e\\n\\n<div style={{\\"background\\": \\"url(/img/ipanema.jpg)\\", \\"backgroundSize\\": \\"150px\\", width: \\"100%\\", height: \\"200px\\", \\"marginTop\\": \\"30px\\", \\"marginBottom\\": \\"30px\\"}}>\\n\\n</div>\\n\\nPostgres is great for a lot of things, however sometimes your data access patterns require a search engine like [Elasticsearch](https://www.elastic.co/). Unfortunately it can be tricky to keep [Postgres](https://www.postgresql.org/) synchronized with Elasticsearch when data changes. Luckily, Statetrace makes it easy.\\n\\nIn this article we will configure a local Postgres instance with Statetrace and pipe all changes into Elasticsearch for easy querying.\\n\\n## Configure the Databases in Docker-Compose\\n\\nI will use the standard docker-compose.yaml from the [tutorial.](https://docs.statetrace.com/docs/intro) with the addition of the elasticsearch container.\\n\\n```yaml\\nversion: \'3\'\\n    \\nservices:\\n  postgres:\\n    image: postgres\\n    environment:\\n      - POSTGRES_DB=postgres\\n      - POSTGRES_USER=postgres\\n      - POSTGRES_PASSWORD=postgres\\n\\n    command:\\n      - \\"postgres\\"\\n      - \\"-c\\"\\n      - \\"wal_level=logical\\"\\n      - \\"-c\\"\\n      - \\"max_wal_senders=2\\"\\n      - \\"-c\\"\\n      - \\"max_replication_slots=2\\"\\n    ports:\\n      - \\"5432:5432\\"\\n\\n  es01:\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.2\\n    container_name: es01\\n    environment:\\n      - node.name=es01\\n      - bootstrap.memory_lock=true\\n      - \\"ES_JAVA_OPTS=-Xms512m -Xmx512m\\"\\n    ulimits:\\n      memlock:\\n        soft: -1\\n        hard: -1\\n    volumes:\\n      - data01:/usr/share/elasticsearch/data\\n    ports:\\n      - 9200:9200\\n    networks:\\n      - elastic\\n  \\n  # The buffer database\\n  statetrace_db:\\n    image: postgres\\n    environment:\\n      - POSTGRES_DB=postgres\\n      - POSTGRES_USER=postgres\\n      - POSTGRES_PASSWORD=postgres\\n\\n    command:\\n      - \\"postgres\\"\\n      - \\"-p\\"\\n      - \\"5433\\"\\n    ports:\\n      - \\"5433:5433\\"\\n```\\n\\n## Configure Statetrace\\n\\nWe will use the following as our configuration for statetrace. \\n\\n```yaml title=\\"STATETRACE_INBOUND_CONFIG\\"\\ninbounds:\\n  - name: Postgres DB\\n    database_type: postgres\\n    username: postgres\\n    password: postgres\\n    database: postgres\\n    host: localhost\\n    publication: \\"statetrace\\"\\n    slot: \\"statetrace\\"\\n    port: 5432\\n    log_new_attributes: true\\n    outbounds:\\n        - name: Webhook\\n          handler: webhook\\n          target_url: \\"http://es01:9200/logs-statetrace_dump/_bulk\\"\\n          middleware: |\\n            fn \\n              %{relation_name: tn} when tn in [\\"my_special_table\\"] -> :ok\\n              _ -> :skip\\n            end\\n          request_builder: |\\n            fn rows ->\\n              payload = \\n                rows\\n                |> Enum.flat_map(fn row -> \\n                    [\\n                        Jason.encode_to_iodata!(%{\\"create\\" => %{}}),\\n                        \\"\\\\n\\",\\n                        Jason.encode_to_iodata!(Map.merge(%{\\"@timestamp\\" => row[\\"row_timestamp\\"]}, row)),\\n                        \\"\\\\n\\"\\n                    ]\\n                end)\\n              %WebhookRequest{method: \\"POST\\", headers: [], payload: payload}\\n            end\\n```\\n\\n### Middleware\\n\\nAn outbound middleware runs on every row and tells whether or not to include it in the outbound request. Elixir\'s pattern matching syntax makes it a breeze to match the tables we want to send to Elasticsearch.\\n\\nIn this example, we will only put rows from `my_special_table` into the index.\\n\\n### Request Builder\\n\\nIn our webhook request, we return a closure that builds our WebhookRequest. The WebhookRequest allows you to set the method, headers and payload of your HTTP request as well as control what fields will be included in the payload. If you want to grab an environment variable, do it outside of the closure.\\n\\nIn Elasticsearch, bulk data is sent as newline-delimited JSON so we will have to format our paylaod accordingly.\\n\\n## Configure Statetrace in Docker-Compose\\n\\nNow we will add Statetrace to our Docker-compose settings\\n\\n### Get your License\\n\\nGo to [https://www.statetrace.com/statetrace-core](https://www.statetrace.com/statetrace-core) to get your free `STATETRACE_LICENSE` key. No email or sign-up required.\\n\\n### Make a secret\\n\\nRun the following command to generate a STATETRACE_SECRET_KEY \\n```bash\\nhead -c 500 /dev/urandom | tr -dc \'a-zA-Z0-9\' | fold -w 32 | head -n 1\\n```\\n\\n### Add to YAML\\n\\nTaking our Statetrace License, our STATETRACE_INBOUND_CONFIG above and tie it all together with the rest of our docker compose file.\\n\\n\\n```yaml title=\\"docker-compose.yaml\\"\\nversion: \'3\'\\n    \\nservices:\\n  postgres:\\n    image: postgres\\n    environment:\\n      - POSTGRES_DB=postgres\\n      - POSTGRES_USER=postgres\\n      - POSTGRES_PASSWORD=postgres\\n\\n    command:\\n      - \\"postgres\\"\\n      - \\"-c\\"\\n      - \\"wal_level=logical\\"\\n      - \\"-c\\"\\n      - \\"max_wal_senders=2\\"\\n      - \\"-c\\"\\n      - \\"max_replication_slots=2\\"\\n    ports:\\n      - \\"5432:5432\\"\\n\\n  es01:\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.2\\n    container_name: es01\\n    environment:\\n      - node.name=es01\\n      - bootstrap.memory_lock=true\\n      - \\"ES_JAVA_OPTS=-Xms512m -Xmx512m\\"\\n    ulimits:\\n      memlock:\\n        soft: -1\\n        hard: -1\\n    volumes:\\n      - data01:/usr/share/elasticsearch/data\\n    ports:\\n      - 9200:9200\\n    networks:\\n      - elastic\\n  \\n  # The buffer database\\n  statetrace_db:\\n    image: postgres\\n    environment:\\n      - POSTGRES_DB=postgres\\n      - POSTGRES_USER=postgres\\n      - POSTGRES_PASSWORD=postgres\\n\\n    command:\\n      - \\"postgres\\"\\n      - \\"-p\\"\\n      - \\"5433\\"\\n    ports:\\n      - \\"5433:5433\\"\\n\\n\\n  statetrace:\\n    image: statetraceofficial/statetrace-beta\\n    environment:\\n      STATETRACE_DATABASE_URL: postgres://postgres:postgres@statetrace_db:5433/postgres\\n      STATETRACE_SECRET_KEY_BASE: \\"123456789123456789123456789123456789123456789123456789123456789123456789\\"\\n      STATETRACE_LICENSE: \\"<statetrace_license>\\"\\n      STATETRACE_INBOUND_CONFIG: |\\n          inbounds:\\n            - name: Postgres DB\\n              database_type: postgres\\n              username: postgres\\n              password: postgres\\n              database: postgres\\n              host: localhost\\n              publication: \\"statetrace\\"\\n              slot: \\"statetrace\\"\\n              port: 5432\\n              log_new_attributes: true\\n              outbounds:\\n                  - name: Webhook\\n                    handler: webhook\\n                    target_url: \\"http://es01:9200/logs-statetrace_dump/_bulk\\"\\n                    middleware: |\\n                      fn \\n                        %{relation_name: tn} when tn in [\\"my_special_table\\"] -> :ok\\n                        _ -> :skip\\n                      end\\n                    request_builder: |\\n                        fn rows ->\\n                          payload = \\n                            rows\\n                            |> Enum.flat_map(fn row -> \\n                                [\\n                                    Jason.encode_to_iodata!(%{\\"create\\" => %{}}),\\n                                    \\"\\\\n\\",\\n                                    Jason.encode_to_iodata!(Map.merge(%{\\"@timestamp\\" => row.row_timestamp}, row)),\\n                                    \\"\\\\n\\"\\n                                ]\\n                            end)\\n                          %WebhookRequest{method: \\"POST\\", headers: [{\\"content-type\\", \\"application/json\\"}], payload: payload}\\n                        end\\n    depends_on:\\n      - statetrace_db\\n      - postgres\\n      - es01\\n```\\n\\nNow all changes happening in the postgres database that match the middleware will be streamed into Elasticsearch under the `logs-statetrace_dump` index.\\n\\n### Conclusion\\n\\nKeeping Postgres synchronized with Elasticsearch doesn\'t need to be hard. We have shown that we can configure the standard webhooks found in Statetrace and use them to push data into the search engine without a lot of configuration.\\n\\nYou can expand on this method to customize the data that you choose to index."},{"id":"redis-server","metadata":{"permalink":"/blog/redis-server","source":"@site/blog/2021-11-09-elixir-redis-server/index.md","title":"Build an Elixir Redis API that\'s 100x faster than HTTP","description":"Build a redis server in Elixir","date":"2021-11-09T00:00:00.000Z","formattedDate":"November 9, 2021","tags":[{"label":"redis","permalink":"/blog/tags/redis"},{"label":"elixir","permalink":"/blog/tags/elixir"},{"label":"api","permalink":"/blog/tags/api"},{"label":"http","permalink":"/blog/tags/http"}],"readingTime":6.16,"truncated":true,"authors":[{"name":"Kyle Hanson","title":"CEO of Statetrace","url":"https://github.com/hansonkd","imageURL":"https://github.com/hansonkd.png","key":"kyle-hanson"}],"prevItem":{"title":"Stream Postgres Changes into Elasticsearch with Statetrace","permalink":"/blog/stream-postgres-elastic-search"},"nextItem":{"title":"Time for a Change - Announcing Statetrace","permalink":"/blog/time-for-a-change"}},"content":"Need a fast server and client? HTTP too slow? Try the Redis Protocol for lightning fast, low-overhead API calls. It\'s easy to implement and nearly every language has mature Redis clients that can connect.\\n\\n\x3c!--truncate--\x3e\\n\\n\x3c!-- ![Docusaurus Plushie](./ipanema.jpg) --\x3e\\n\\n<div style={{\\"background\\": \\"url(/img/ipanema.jpg)\\", \\"backgroundSize\\": \\"150px\\", width: \\"100%\\", height: \\"200px\\", \\"marginTop\\": \\"30px\\", \\"marginBottom\\": \\"30px\\"}}>\\n\\n</div>\\n\\n*This project was inspired by <a href=\\"https://github.com/hansonkd/Tino/\\">Tino</a>, the Redis/MsgPack framework for Python.*\\n\\nBuilding a server based on Redis Protocol from scratch can sound intimidating. But if you know what you are doing it can be relatively straight forward to implement. A huge shortcut involves using [Redix](https://github.com/whatyouhide/redix) to parse the binary stream in a fast and efficient manner.\\n\\nIn this article we will implement a Redis echo server and explain how to extend the server to handle your own custom commands. In the end, it results in a performance boost of over **100x** using Redis instead of HTTP.\\n\\nThis blog post assumes you are familar with Elixir and its Application structure. If you want to learn more about TCP connections and supervision, read the [official Elixir article](https://elixir-lang.org/getting-started/mix-otp/task-and-gen-tcp.html).\\n\\n## Advantages\\n\\nFirst before we get started, lets discuss some of the advantages. Building a server based on RESP (the Redis protocol) means you are cutting out a lot of overhead associated with HTTP. In addtion to a more lean protocol, nearly every language has a high-performance client built for Redis that allows pipelining. Pipelining combines commands as you send them for even greater efficiency. Most redis clients even support pooling for working at high concurrencies.\\n\\nWith these built in features, you don\'t have to do very much to talk to your server in an extremely high performance fashion. \\n\\n\\n## Reading the TCP connection\\n\\nTo get started building our server, we will need to accept a TCP connection. We do this by looping over `:gen_tcp.accept` and spawning a task.\\n\\n```elixir\\ndefmodule MyRedisServer.Redis do\\n  require Logger\\n  def accept(port) do\\n    {:ok, socket} = :gen_tcp.listen(port, [:binary, active: false, reuseaddr: true])\\n    Logger.info(\\"Accepting connections on port #{port}\\")\\n    loop_acceptor(socket)\\n  end\\n\\n  defp loop_acceptor(socket) do\\n    {:ok, client} = :gen_tcp.accept(socket)\\n\\n    {:ok, pid} =\\n      Task.start(fn ->\\n        serve(client, %{continuation: nil})\\n      end)\\n\\n    :ok = :gen_tcp.controlling_process(client, pid)\\n\\n    loop_acceptor(socket)\\n  end\\nend\\n```\\n\\nNow we are ready to read packets from the connection. Elixir\'s Redis client Redix includes an parser for us to use.\\n\\n```elixir\\ndefmodule MyRedisServer.Redis do\\n  ...\\n\\n  defp serve(socket, %{continuation: nil}) do\\n    case :gen_tcp.recv(socket, 0) do\\n      {:ok, data} ->  handle_parse(socket, Redix.Protocol.parse(data))\\n      {:error, :closed} -> :ok\\n    end\\n  end\\n\\n  defp serve(socket, %{continuation: fun}) do\\n    case :gen_tcp.recv(socket, 0) do\\n      {:ok, data} ->  handle_parse(socket, fun.(data))\\n      {:error, :closed} -> :ok\\n    end\\n  end\\nemd\\n```\\n\\nHandling the parse result is straight forward. Either an entire message was processed and we can handle it, and respond, or a partial message was recieved and we need to wait for more data.\\n\\n\\n```elixir\\ndefmodule MyRedisServer.Redis do\\n  ...\\n\\n  defp handle_parse(socket, {:continuation, fun}) do\\n    serve(socket, %{continuation: fun})\\n  end\\n\\n  defp handle_parse(socket, {:ok, req, left_over}) do\\n    resp = handle(req)\\n\\n    :gen_tcp.send(socket, Redix.Protocol.pack(resp))\\n\\n    case left_over do\\n      \\"\\" -> serve(socket, %{continuation: nil})\\n      _ -> handle_parse(socket, Redix.Protocol.parse(left_over))\\n    end\\n  end\\n\\n  def handle(data) do\\n    data\\n  end\\nend\\n```\\n\\n## Complete example\\n\\nFinally we are ready to put it all together. All the pieces come together to form a nice little echo server.\\n\\n```elixir\\ndefmodule MyRedisServer.Redis do\\n  require Logger\\n\\n  def accept(port) do\\n    {:ok, socket} = :gen_tcp.listen(port, [:binary, active: false, reuseaddr: true])\\n    Logger.info(\\"Accepting connections on port #{port}\\")\\n    loop_acceptor(socket)\\n  end\\n\\n  defp loop_acceptor(socket) do\\n    {:ok, client} = :gen_tcp.accept(socket)\\n\\n    {:ok, pid} =\\n      Task.start(fn ->\\n        serve(client, %{continuation: nil})\\n      end)\\n\\n    :ok = :gen_tcp.controlling_process(client, pid)\\n\\n    loop_acceptor(socket)\\n  end\\n\\n  defp serve(socket, %{continuation: nil}) do\\n    case :gen_tcp.recv(socket, 0) do\\n      {:ok, data} ->  handle_parse(socket, Redix.Protocol.parse(data))\\n      {:error, :closed} -> :ok\\n    end\\n  end\\n\\n  defp serve(socket, %{continuation: fun}) do\\n    case :gen_tcp.recv(socket, 0) do\\n      {:ok, data} ->  handle_parse(socket, fun.(data))\\n      {:error, :closed} -> :ok\\n    end\\n  end\\n\\n  defp handle_parse(socket, {:continuation, fun}) do\\n    serve(socket, %{continuation: fun})\\n  end\\n\\n  defp handle_parse(socket, {:ok, req, left_over}) do\\n    resp = handle(req)\\n\\n    :gen_tcp.send(socket, Redix.Protocol.pack(resp))\\n\\n    case left_over do\\n      \\"\\" -> serve(socket, %{continuation: nil})\\n      _ -> handle_parse(socket, Redix.Protocol.parse(left_over))\\n    end\\n  end\\n\\n  def handle(data) do\\n    data\\n  end\\nend\\n\\n```\\n\\n\\nRun this server in your Application\'s supervision tree:\\n\\n\\n```elixir\\n defmodule MyRedisServer.Application do\\n  use Application\\n\\n  ...\\n\\n  def start(_type, _args) do\\n    claims = get_license_claims!()\\n\\n    children = [\\n      ...,\\n      Supervisor.child_spec({Task, fn -> MyRedisServer.Redis.accept(3211) end},   restart: :permanent)\\n    ]\\n\\n    ...\\n\\n    Supervisor.start_link(children, opts)\\n  end\\nend\\n```\\n\\n## Connecting from a client\\n\\nStart your mix project and you should be able to connect to redis on 3211 and the command should echo what you send it.\\n\\n```elixir\\n> {:ok, conn} = Redix.start_link(\\"redis://localhost:3211\\")\\n> Redix.command(conn, [\\"COOL_COMMAND\\", \\"123\\"])\\n{:ok, [\\"COOL_COMMAND\\", \\"123\\"]}\\n```\\n\\nAdding commands to your new redis server is easy with pattern matching:\\n\\n```elixir\\ndefmodule MyRedisServer.Redis do\\n  ...\\n\\n  def handle([\\"PUT\\", key, val]) do\\n    Cachex.put(:my_cachex, key, val)\\n    [\\"OK\\"]\\n  end\\n\\n  def handle([\\"GET\\", key]) do\\n    [Cachex.get(:my_cachex, key)]\\n  end\\n\\n  def handle([\\"ECHO\\", msg]) do\\n    msg\\n  end\\n\\n  def handle(_data) do\\n    %Redix.Error{message: \\"UNKNOWN_COMMAND\\"}\\n  end\\nend\\n```\\n\\n## MsgPack\\n\\nMsgPack is essentially a faster, more compact version of JSON. Use it to serialize complex structures into binary data to pass back and forth between your API.\\n\\n\\n```elixir\\ndefmodule MyRedisServer.Redis do\\n  ...\\n\\n  def handle([command, payload]) do\\n    case handle_command(command, MsgPax.unpack!(payload)) do\\n        {:error, e} -> %Redix.Error{message: \\"ERROR #{e}\\"}\\n        value -> [MsgPax.pack!(value)]\\n    end\\n  end\\n\\n  def hande(_) do\\n    %Redix.Error{message: \\"INMVALID_FORMAT\\"}\\n  end\\n\\n  defp handle_command(\\"PUT\\", [key, val]) do\\n    Cachex.put(:my_cachex, key, val)\\n    [\\"OK\\"]\\n  end\\n\\n  defp handle_command(\\"GET\\", key) do\\n    Cachex.get(:my_cachex, key)\\n  end\\n\\n  defp handle_command(\\"ECHO\\", msg) do\\n    msg\\n  end\\n\\n  defp handle_command(_command, _data) do\\n    {:error, \\"INVALID_COMMAND\\"}\\n  end\\nend\\n```\\n\\n## Benchmark\\n\\nFor this benchmark we will compare HTTP Phoenix to our Redis Server.\\n\\nOur HTTP Phoenix Controllers:\\n\\n```elixir\\n  # GET -> Text\\n  def bench(conn, %{\\"payload\\" => payload, \\"times\\" => times}) when is_binary(times) do\\n    text(conn, String.duplicate(payload, String.to_integer(times)))\\n  end\\n\\n  # POST -> JSON\\n  def bench(conn, %{\\"payload\\" => payload, \\"times\\" => times}) do\\n    json(conn, %{\\"data\\" => String.duplicate(payload, times)})\\n  end\\n```\\n\\nand our Redis server:\\n\\n```elixir\\n  def handle([\\"BENCH\\", payload, number]) do\\n    [String.duplicate(payload, String.to_integer(number))]\\n  end\\n```\\n\\nWe will use [Finch](https://github.com/sneako/finch) for the HTTP client, which labels itself as \\"performance focused\\".\\n\\nFor the full benchmark see [the source](https://gist.github.com/hansonkd/cd34329fe4f346e680b39a17d9988af4).\\n\\nWe will remotely call our functions using the Finch HTTP pool, a single Redix connection, or a pool of Redix connections. We will also test pipelining vs calling each command individually for Redix. We will call our remote function 1000 times concurrently and ask it to duplicate the string `\\"12345&?\\\\\\"678,\\\\n90\\"`  100 times and respond.\\n\\n\\n```bash\\nName                           ips        average  deviation         median         99th %\\nredix_pool                   70.44       14.20 ms    \xb136.07%       13.30 ms       50.60 ms\\nrun_redix_pipeline           30.56       32.73 ms    \xb165.74%       47.26 ms       91.99 ms\\nredix_pool_pipelined         21.55       46.40 ms     \xb13.87%       47.59 ms       48.12 ms\\nredix                        13.84       72.28 ms     \xb19.91%       72.09 ms       80.31 ms\\nfinch_get                     0.55     1814.88 ms     \xb12.44%     1814.88 ms     1846.24 ms\\nfinch_post                    0.54     1859.71 ms     \xb10.70%     1859.71 ms     1868.97 ms\\n```\\n\\n\\nThe results show that running Redis protocol is well over 100x faster than relying on HTTP. By default Phoenix sends extra headers for the content type and other information. In addition there is extra overhead encoding and decoding the values for URL encoding and JSON.\\n\\nOverall using Redis as a Protocol instead of HTTP results in orders of magnitude higher troughput.\\n\\n## Conclusion\\n\\nWe wrote a high-performance server based on the Redis Protocol in around 10 minutes. This server can handle thousands of connections easily and has minimal overhead. One downside is that load balancing becomes more of a challenge when doing multi-node deploys when using a protocol other than HTTP.\\n\\nIf you have a one or thousands of clients that need to communicate with a server in the fastest way possible, consider using Redis as your protocol of choice instead of HTTP."},{"id":"time-for-a-change","metadata":{"permalink":"/blog/time-for-a-change","source":"@site/blog/2021-11-08-time-for-a-change/index.md","title":"Time for a Change - Announcing Statetrace","description":"Data auditing with Statetrace","date":"2021-11-08T00:00:00.000Z","formattedDate":"November 8, 2021","tags":[{"label":"statetrace","permalink":"/blog/tags/statetrace"},{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"mysql","permalink":"/blog/tags/mysql"},{"label":"auditing","permalink":"/blog/tags/auditing"}],"readingTime":4.095,"truncated":true,"authors":[{"name":"Kyle Hanson","title":"CEO of Statetrace","url":"https://github.com/hansonkd","imageURL":"https://github.com/hansonkd.png","key":"kyle-hanson"}],"prevItem":{"title":"Build an Elixir Redis API that\'s 100x faster than HTTP","permalink":"/blog/redis-server"}},"content":"The world of application data today is broken. We treat our most valuable asset as a second-class citizen, only keeping the most recent version data available and throwing away critical information of how the data got there. It\'s is a hard problem to fix. Statetrace is here to make it easier.\\n\\n\x3c!--truncate--\x3e\\n\\n\x3c!-- ![Docusaurus Plushie](./ipanema.jpg) --\x3e\\n\\n<div style={{\\"background\\": \\"url(/img/ipanema.jpg)\\", \\"backgroundSize\\": \\"150px\\", width: \\"100%\\", height: \\"200px\\", \\"marginTop\\": \\"30px\\", \\"marginBottom\\": \\"30px\\"}}>\\n\\n</div>\\n\\nData auditing is the future of application development. It enables teams to solve tougher problems faster. With Statetrace, teams can develop an auditing solution in hours instead of months and start delivering reliable answers to their customers. Statetrace annotates row level changes from your databases, piping them into webhooks, into data warehouses, or indexing them for fast searches.\\n\\n\\n## Whats wrong today\\n\\nThe world of auditing today has been stuck for decades. There exist [libraries](https://django-simple-history.readthedocs.io/en/latest/) and [tools](https://github.com/etianen/django-reversion) at the application layer to associate changes with who changed them, however these tools are deeply flawed. Because they work at the application layer, they miss things that don\'t go through the application; like migrations or someone connected directly to the DB. More importantly they are strictly framework dependent and do not offer a general solution.\\n\\nCDC pipelines capture the changes accurately but do not associate those changes with application meta information.\\n\\n\\n## How it works\\n\\nStatetrace connects to the [logical replication](https://www.postgresql.org/docs/10/logical-replication.html) of Postgres or the [BinLog](https://dev.mysql.com/doc/internals/en/binary-log-overview.html) of MySQL. By reading events directly from the replication log, Statetrace gets a 100% accurate history of your data. More importantly because it is intefacing with the database instead of the application, Statetrace can work with any framework or language with minimal configuration.\\n\\n\\nThe application annotates transactions by writing to an annotations table in the same transaction that you change other data, associating session and user information with individual row changes.\\n\\n\\n## Solving the \\"Who dunnit?\\"\\n\\nNobody wants to answer the dreaded customer complaint \\"Who changed my data?\\" Even if the customer was the last one to change the data, without a proper auditing solution one might not be able to give a reliable answer. Having uncertainty around the history of data puts a company\'s reputation at risk.\\n\\nStatetrace puts the answers to these questions at your fingertips. With an annotated audit log, each row change is associated with meta-information about who in the application changed the data. Pipe these changes into the destination of your choice for easy searching.\\n\\n\\n\\n## Github for Code. Statetrace for Data.\\n\\n\\nVersion control for code is integral to the development process. Companies spend billions of dollars every year on developer salaries and want to keep that investment of developer output. Data about what the code was and who changed it is so valuable that a multi-billion dollar industry has grown to support those needs. \\n\\nHowever, companies are throwing away money when it comes to their actual bread and butter: the application data. The stream of changes from application data are a gold mine for solving problems and answering questions. But the vast majority of companies today throw away these changes, because they have little value as they don\'t connect the change to who made the change. This is bad, because you don\'t know what type of questions you might want to know in the future and once you throw it away, its gone for ever.\\n\\nStatetrace is solving this problem. Statetrace makes the stream of changes useful by associating the change with who changed it and simplifying piping these changes into other data sources.\\n\\n## Time traveling SQL\\n\\nOnce all changes from a DB are collected, they can be used to recreate transaction-level point-in-time snapshots of your entire database, a particular table, or just a single row. This allows you to easily go back to see what a the result of a query was. It also helps you answer more interesting questions in data analytics as you can compare two points in time in a single query all in your existing data warehouse.\\n\\n\\n## Compliance focused\\n\\nYour data belongs with you. Statetrace is designed to be run on-prem, leaving you in complete control of your data. Whether you are running a HIPAA deployment or need to stay SOC 2 compliant, Statetrace works with your compliance team to succeed.\\n\\n\\n## Statetrace Core\\n\\nUsers can try [Statetrace Core](/docs/intro) today for free. Its a limited edition of statetrace without a UI, but with all of the power. Try it out locally to quickly connect your database and start scanning within minutes.\\n\\n\\n## Statetrace Enterprise\\n\\nOur flagship product is [Statetrace for Enterprise](https://www.statetrace.com). Its the full featured Statetrace experience with a robust UI, enterprise level user permissions, pre-constructed SQL models for time-travel and support from our customer success team.\\n\\n## The future\\n\\nWe are developing the highest quality auditing experience. If what we are doing sounds interesting, reach out at [hello@statetrace.com](mailto:hello@statetrace.com) and we would love to tell you about what we are working on."}]}')}}]);