<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://docs.statetrace.com/blog</id>
    <title>Statetrace Blog</title>
    <updated>2021-11-16T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://docs.statetrace.com/blog"/>
    <subtitle>Statetrace Blog</subtitle>
    <icon>https://docs.statetrace.com/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Build a Postgres Proxy in Elixir using Pattern Matching]]></title>
        <id>build-a-postgres-proxy</id>
        <link href="https://docs.statetrace.com/blog/build-a-postgres-proxy"/>
        <updated>2021-11-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Build a postgres proxy in Elixir]]></summary>
        <content type="html"><![CDATA[<p>Want to learn how your Application talks to your Database? Build a proxy using Elixir with its powerful pattern matching and <code>gen_tcp</code> to take your database understanding to the next level. In this article we build a Postgres proxy in Elixir to show all you need is a little curiousity to master one of the most popular SQL protocols around.</p><div style="background:url(/img/ipanema.jpg);background-size:150px;width:100%;height:200px;margin-top:30px;margin-bottom:30px"></div><p>Building a proxy can be a great way to understand a protocol. Everything you build doesn&#x27;t need to be used in production in order for it to be valuable to your understanding of the tech. The better you understand the tech, the more powerful the tech becomes.</p><p>In this post, we will explore how to accept a socket and proxy it to another, while at the same time parsing the stream. At the end of the post we can use our proxy to control which queries get executed on our postgres server.</p><h2>Build the Server</h2><p>To build this proxy, I used the postgres <a href="https://www.postgresql.org/docs/9.3/protocol.html">reference documentation</a> regarding the protocol.</p><p>This blog post assumes you are familar with Elixir and its Application structure. If you want to learn more about TCP connections and supervision, read the <a href="https://elixir-lang.org/getting-started/mix-otp/task-and-gen-tcp.html">official Elixir article</a>.</p><h3>Accepting the socket</h3><p>For this proxy we will open a TCP socket in active mode. In order to understand what active mode is, it&#x27;s helpful to know what it isn&#x27;t. When active is false, the VM will stop reading packets from the socket until you call  <code>:gen_tcp.recv</code>. When the socket is set to active mode, the VM instead reads data as fast as possible from the socket and uses the processes mailbox as the buffer.</p><p>Because we will be reading data as fast as possible from both upstream and downstream, we will need to split out our listener into two processes. One process will read upstream data, parse it into commands, and then forward those commands to the Postgres database. The other process will read the responses from the Postgres database and forward them directly back to the client.</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do
  require Logger

  def accept(port) do
    {:ok, socket} =
      :gen_tcp.listen(port, [:binary, active: true, reuseaddr: true, packet: 0, nodelay: true])

    Logger.info(&quot;Accepting connections on port #{port}&quot;)
    loop_acceptor(socket)
  end

  defp loop_acceptor(socket) do
    {:ok, client} = :gen_tcp.accept(socket)

    {:ok, pid} =
      Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn -&gt;
        {:ok, outbound} =
          :gen_tcp.connect(&#x27;localhost&#x27;, 5432, [:binary, active: true, packet: 0, nodelay: true])

        {:ok, pid2} =
          Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn -&gt;
            serve_downstream(client, outbound)
          end)

        :ok = :gen_tcp.controlling_process(outbound, pid2)

        serve_upstream(client, outbound, nil, true)
      end)

    :ok = :gen_tcp.controlling_process(client, pid)

    loop_acceptor(socket)
  end

end

</code></pre><p>We use <code>:gen_tcp.connect</code> to connect to the real postgres database and spin it off into a loop of its own to pipe responses back to the client.</p><h4>Serving upstream</h4><p>Now build the parser for the upstream connection</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do

  ...

  def serve_upstream(socket, outbound, nil, is_first) do
    data = socket |&gt; read_line()

    handle_parse(socket, outbound, parse_msg(data, is_first))
  end

  def serve_upstream(socket, outbound, fun, _is_first) do
    data = socket |&gt; read_line()

    r = fun.(data)

    handle_parse(socket, outbound, r)
  end

  def read_line(socket) do
    receive do
      {:tcp, ^socket, data} -&gt; data
    end
  end
end
</code></pre><p>To parse the message is very simple. It is a very simple length-prefixed binary format with a message tag byte. The very first message of a connection is the only exception and excludes the tag. There is a chance that the current data we have is not enough to satisfy the length of the message. In this case we will use a continuation so that the next data that comes in, we can check to see if it completes the message.</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do

  ...

  # On the first message don&#x27;t extract the tag
  defp parse_msg(bin, true) do
    # Use pattern matching to extract the length
    &lt;&lt;len::unsigned-integer-32, _other_rest::binary&gt;&gt; = bin

    case bin do
      # Pattern match to see if our binary is big enough
      &lt;&lt;msg_body::binary-size(len), final_rest::binary&gt;&gt; -&gt;
        {:ok, {{:msgStartup, nil}, msg_body}, final_rest}

      _ -&gt;
        {:continuation,
         fn data -&gt;
           handle_continuation(len, {:msgStartup, nil}, bin, data)
         end}
    end
  end

  # Pattern match the binary to extract the tag.
  defp parse_msg(&lt;&lt;c::size(8), rest::binary&gt;&gt;, false) do
    tag = tag_to_msg_type(c)

    # Use pattern matching to extract the length
    &lt;&lt;len::unsigned-integer-32, _other_rest::binary&gt;&gt; = rest

    case rest do
      # Pattern match to see if our binary is big enough
      &lt;&lt;msg_body::binary-size(len), final_rest::binary&gt;&gt; -&gt;
        {:ok, {{tag, c}, msg_body}, final_rest}

      _ -&gt;
        {:continuation,
         fn data -&gt;
           handle_continuation(len, {tag, c}, rest, data)
         end}
    end
  end

  def handle_continuation(l, tag, other, data) do
    new_data = other &lt;&gt; data

    case new_data do
      &lt;&lt;msg_body::binary-size(l), rest::binary&gt;&gt; -&gt;
        {:ok, {tag, msg_body}, rest}

      _ -&gt;
        {:continuation,
         fn data -&gt;
           handle_continuation(l, tag, new_data, data)
         end}
    end
  end
end
</code></pre><p>The first byte in non-connection messages is a tag. We will convert this tag into an atom.</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do

  ...

  defp tag_to_msg_type(val) do
    case val do
      ?1 -&gt; :msgParseComplete
      ?2 -&gt; :msgBindComplete
      ?3 -&gt; :msgCloseComplete
      ?A -&gt; :msgNotificationResponse
      ?c -&gt; :msgCopyDone
      ?C -&gt; :msgCommandComplete
      ?d -&gt; :msgCopyData
      ?D -&gt; :msgDataRow
      ?E -&gt; :msgErrorResponse
      ?f -&gt; :msgFail
      ?G -&gt; :msgCopyInResponse
      ?H -&gt; :msgCopyOutResponse
      ?I -&gt; :msgEmptyQueryResponse
      ?K -&gt; :msgBackendKeyData
      ?n -&gt; :msgNoData
      ?N -&gt; :msgNoticeResponse
      ?R -&gt; :msgAuthentication
      ?s -&gt; :msgPortalSuspended
      ?S -&gt; :msgParameterStatus
      ?t -&gt; :msgParameterDescription
      ?T -&gt; :msgRowDescription
      ?p -&gt; :msgPasswordMessage
      ?W -&gt; :CopyBothResponse
      ?Q -&gt; :msgQuery
      ?X -&gt; :msgTerminate
      ?Z -&gt; :msgReadyForQuery
      ?P -&gt; :msgParse
      ?B -&gt; :msgBind
      _ -&gt; :msgNoTag
    end
  end
end
</code></pre><p>Finally we will handle the parse result. </p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do

  ...

  def handle_parse(socket, outbound, {:continuation, continuation}) do
    serve_upstream(socket, outbound, continuation, false)
  end

  def handle_parse(socket, outbound, {:ok, {{_msgType, c}, data}, left_over}) do
    to_send =
      case c do
        nil -&gt; data
        _ -&gt; [c, data]
      end

    :ok = :gen_tcp.send(outbound, to_send)

    case left_over do
      &quot;&quot; -&gt; serve_upstream(socket, outbound, nil, false)
      _ -&gt; handle_parse(socket, outbound, parse_msg(left_over, false))
    end
  end
end
</code></pre><h3>Serving downstream</h3><p>The downstream response is even simpler. We will not parse the message and simply forward the data directly to the socket.</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do

  ...

  def serve_downstream(socket, outbound) do
    data = outbound |&gt; read_line()
    :ok = :gen_tcp.send(socket, data)

    serve_downstream(socket, outbound)
  end
end
</code></pre><h3>Complete server</h3><p>The complete server is less than 200 lines of code.</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do
  require Logger

  def accept(port) do
    {:ok, socket} =
      :gen_tcp.listen(port, [:binary, active: true, reuseaddr: true, packet: 0, nodelay: true])

    Logger.info(&quot;Accepting connections on port #{port}&quot;)
    loop_acceptor(socket)
  end

  defp loop_acceptor(socket) do
    {:ok, client} = :gen_tcp.accept(socket)

    {:ok, pid} =
      Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn -&gt;
        {:ok, outbound} =
          :gen_tcp.connect(&#x27;localhost&#x27;, 5432, [:binary, active: true, packet: 0, nodelay: true])

        {:ok, pid2} =
          Task.Supervisor.start_child(Statetrace.TaskSupervisor, fn -&gt;
            serve_downstream(client, outbound)
          end)

        :ok = :gen_tcp.controlling_process(outbound, pid2)

        serve_upstream(client, outbound, nil, true)
      end)

    :ok = :gen_tcp.controlling_process(client, pid)

    loop_acceptor(socket)
  end

  defp serve_upstream(socket, outbound, nil, is_first) do
    data = socket |&gt; read_line()

    handle_parse(socket, outbound, parse_msg(data, is_first))
  end

  defp serve_upstream(socket, outbound, fun, _is_first) do
    data = socket |&gt; read_line()

    r = fun.(data)

    handle_parse(socket, outbound, r)
  end

  defp handle_parse(socket, outbound, {:continuation, continuation}) do
    serve_upstream(socket, outbound, continuation, false)
  end

  defp handle_parse(socket, outbound, {:ok, {{_msgType, c}, data}, left_over}) do
    to_send =
      case c do
        nil -&gt; data
        _ -&gt; [c, data]
      end

    :ok = :gen_tcp.send(outbound, to_send)

    case left_over do
      &quot;&quot; -&gt; serve_upstream(socket, outbound, nil, false)
      _ -&gt; handle_parse(socket, outbound, parse_msg(left_over, false))
    end
  end

  defp serve_downstream(socket, outbound) do
    data = outbound |&gt; read_line()
    :ok = :gen_tcp.send(socket, data)

    serve_downstream(socket, outbound)
  end

  defp read_line(socket) do
    receive do
      {:tcp, ^socket, data} -&gt; data
    end
  end

  defp tag_to_msg_type(val) do
    case val do
      ?1 -&gt; :msgParseComplete
      ?2 -&gt; :msgBindComplete
      ?3 -&gt; :msgCloseComplete
      ?A -&gt; :msgNotificationResponse
      ?c -&gt; :msgCopyDone
      ?C -&gt; :msgCommandComplete
      ?d -&gt; :msgCopyData
      ?D -&gt; :msgDataRow
      ?E -&gt; :msgErrorResponse
      ?f -&gt; :msgFail
      ?G -&gt; :msgCopyInResponse
      ?H -&gt; :msgCopyOutResponse
      ?I -&gt; :msgEmptyQueryResponse
      ?K -&gt; :msgBackendKeyData
      ?n -&gt; :msgNoData
      ?N -&gt; :msgNoticeResponse
      ?R -&gt; :msgAuthentication
      ?s -&gt; :msgPortalSuspended
      ?S -&gt; :msgParameterStatus
      ?t -&gt; :msgParameterDescription
      ?T -&gt; :msgRowDescription
      ?p -&gt; :msgPasswordMessage
      ?W -&gt; :CopyBothResponse
      ?Q -&gt; :msgQuery
      ?X -&gt; :msgTerminate
      ?Z -&gt; :msgReadyForQuery
      ?P -&gt; :msgParse
      ?B -&gt; :msgBind
      _ -&gt; :msgNoTag
    end
  end

  defp parse_msg(bin, true) do
    &lt;&lt;len::unsigned-integer-32, _other_rest::binary&gt;&gt; = bin

    case bin do
      &lt;&lt;msg_body::binary-size(len), final_rest::binary&gt;&gt; -&gt;
        {:ok, {{:msgStartup, nil}, msg_body}, final_rest}

      _ -&gt;
        {:continuation,
         fn data -&gt;
           handle_continuation(len, {:msgStartup, nil}, bin, data)
         end}
    end
  end

  defp parse_msg(&lt;&lt;c::size(8), rest::binary&gt;&gt;, false) do
    tag = tag_to_msg_type(c)

    &lt;&lt;len::unsigned-integer-32, _other_rest::binary&gt;&gt; = rest

    case rest do
      &lt;&lt;msg_body::binary-size(len), final_rest::binary&gt;&gt; -&gt;
        {:ok, {{tag, c}, msg_body}, final_rest}

      _ -&gt;
        {:continuation,
         fn data -&gt;
           handle_continuation(len, {tag, c}, rest, data)
         end}
    end
  end

  defp handle_continuation(l, tag, other, data) do
    new_data = other &lt;&gt; data

    case new_data do
      &lt;&lt;msg_body::binary-size(l), rest::binary&gt;&gt; -&gt;
        {:ok, {tag, msg_body}, rest}

      _ -&gt;
        {:continuation,
         fn data -&gt;
           handle_continuation(l, tag, new_data, data)
         end}
    end
  end
end
</code></pre><p>To start it with your application, add it to your supervisor tree:</p><pre><code class="language-elixir">defmodule Statetrace.Application do
  use Application

  def start(_type, _args) do

    children = [
      {Task.Supervisor, name: Statetrace.TaskSupervisor},
      Supervisor.child_spec({Task, fn -&gt; Statetrace.PostgresProxy.accept(5433) end},
        restart: :permanent
      )

    ]
    opts = [strategy: :one_for_one, name: Statetrace.Supervisor]
    Supervisor.start_link(children, opts)
  end
end
</code></pre><h3>Connecting</h3><p>Now you can connect to your proxy from postgres clients in every language!</p><p>Elixir:</p><pre><code class="language-elixir">{:ok, conn} = Postgrex.start_link(host: &quot;localhost&quot;, port: 5433, password: &quot;postgres&quot;, username: &quot;postgres&quot;, database: &quot;postgres&quot;)
Postgrex.query!(conn, &quot;SELECT 1;&quot;, [])
</code></pre><p>In Python:</p><pre><code class="language-python">conn = psycopg2.connect(&quot;dbname=postgres host=localhost port=5433 user=postgres password=postgres&quot;)
cur = conn.cursor()
cur.execute(&quot;SELECT 1;&quot;)
</code></pre><h2>Controlling Postgres Statements</h2><p>So you&#x27;re probabaly asking what can we do with this proxy? If we change our <code>handle_parse</code> function we can &quot;police&quot; the queries made to the upstream server. The format of the query message is simple, it is the query string you submitted with a null byte.</p><pre><code class="language-elixir">defmodule Statetrace.PostgresProxy do
  ...

  defp is_query_good?(q) do
    # In reality you will want something more secure than this
    String.starts_with?(q, &quot;SELECT&quot;)
  end

  # Use pattern matching to process :msgQuery differently.
  # The query is prefixed with the length and suffixed with a null byte
  defp handle_parse(socket, outbound, {:ok, {{:msgQuery, _c}, &lt;&lt;_len::unsigned-integer-32, data::binary&gt;&gt;}, _left_over} = msg) do
    query = String.trim_trailing(data, &lt;&lt;0&gt;&gt;)
    Logger.info(&quot;Query: #{query}&quot;)

    if is_query_good?(query) do
      do_handle_parse(socket, outbound, msg)
    else
      raise &quot;Unauthorized Query&quot;
    end
  end

  defp handle_parse(socket, outbound, msg) do
    do_handle_parse(socket, outbound, msg)
  end

  defp do_handle_parse(socket, outbound, {:ok, {{_msgType, c}, data}, left_over}) do
    to_send =
      case c do
        nil -&gt; data
        _ -&gt; [c, data]
      end

    :ok = :gen_tcp.send(outbound, to_send)

    case left_over do
      &quot;&quot; -&gt; serve_upstream(socket, outbound, nil, false)
      _ -&gt; handle_parse(socket, outbound, parse_msg(left_over, false))
    end
  end

  defp do_handle_parse(socket, outbound, {:continuation, continuation}) do
    serve_upstream(socket, outbound, continuation, false)
  end
end
</code></pre><h2>Conclusion</h2><p>There is still more to the protocol, but we have shown that with a little patience you can produce a useful proxy that understands the protocol in less than 200 lines of code. Elixir&#x27;s pattern matching simplifies extracting information from the binary stream and <code>gen_tcp</code> makes it simple to deal with sockets.</p><p>Like what you read? Reach out at <a href="mailto:hello@statetrace.com">hello@statetrace.com</a> to learn more about what we are working on.</p>]]></content>
        <author>
            <name>Kyle Hanson</name>
            <uri>https://github.com/hansonkd</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stream Postgres Changes into Elasticsearch with Statetrace]]></title>
        <id>stream-postgres-elastic-search</id>
        <link href="https://docs.statetrace.com/blog/stream-postgres-elastic-search"/>
        <updated>2021-11-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Stream Postgres Changes into Elasticsearch with Statetrace]]></summary>
        <content type="html"><![CDATA[<p>Need to connect Postgres to Elasticsearch? Statetrace makes it easy to stream changes from your application database into the popular search engine.</p><div style="background:url(/img/ipanema.jpg);background-size:150px;width:100%;height:200px;margin-top:30px;margin-bottom:30px"></div><p>Postgres is great for a lot of things, however sometimes your data access patterns require a search engine like <a href="https://www.elastic.co/">Elasticsearch</a>. Unfortunately it can be tricky to keep <a href="https://www.postgresql.org/">Postgres</a> synchronized with Elasticsearch when data changes. Luckily, Statetrace makes it easy.</p><p>In this article we will configure a local Postgres instance with Statetrace and pipe all changes into Elasticsearch for easy querying.</p><h2>Configure the Databases in Docker-Compose</h2><p>I will use the standard docker-compose.yaml from the <a href="https://docs.statetrace.com/docs/intro">tutorial.</a> with the addition of the elasticsearch container.</p><pre><code class="language-yaml">version: &#x27;3&#x27;
    
services:
  postgres:
    image: postgres
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

    command:
      - &quot;postgres&quot;
      - &quot;-c&quot;
      - &quot;wal_level=logical&quot;
      - &quot;-c&quot;
      - &quot;max_wal_senders=2&quot;
      - &quot;-c&quot;
      - &quot;max_replication_slots=2&quot;
    ports:
      - &quot;5432:5432&quot;

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.2
    container_name: es01
    environment:
      - node.name=es01
      - bootstrap.memory_lock=true
      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - elastic
  
  # The buffer database
  statetrace_db:
    image: postgres
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

    command:
      - &quot;postgres&quot;
      - &quot;-p&quot;
      - &quot;5433&quot;
    ports:
      - &quot;5433:5433&quot;
</code></pre><h2>Configure Statetrace</h2><p>We will use the following as our configuration for statetrace. </p><pre><code class="language-yaml" metastring="title=&quot;STATETRACE_INBOUND_CONFIG&quot;" title="&quot;STATETRACE_INBOUND_CONFIG&quot;">inbounds:
  - name: Postgres DB
    database_type: postgres
    username: postgres
    password: postgres
    database: postgres
    host: localhost
    publication: &quot;statetrace&quot;
    slot: &quot;statetrace&quot;
    port: 5432
    log_new_attributes: true
    outbounds:
        - name: Webhook
          handler: webhook
          target_url: &quot;http://es01:9200/logs-statetrace_dump/_bulk&quot;
          middleware: |
            fn 
              %{relation_name: tn} when tn in [&quot;my_special_table&quot;] -&gt; :ok
              _ -&gt; :skip
            end
          request_builder: |
            fn rows -&gt;
              payload = 
                rows
                |&gt; Enum.flat_map(fn row -&gt; 
                    [
                        Jason.encode_to_iodata!(%{&quot;create&quot; =&gt; %{}}),
                        &quot;\n&quot;,
                        Jason.encode_to_iodata!(Map.merge(%{&quot;@timestamp&quot; =&gt; row[&quot;row_timestamp&quot;]}, row)),
                        &quot;\n&quot;
                    ]
                end)
              %WebhookRequest{method: &quot;POST&quot;, headers: [{&quot;content-type&quot;, &quot;application/json&quot;}], payload: payload}
            end
</code></pre><h3>Middleware</h3><p>An outbound middleware runs on every row and tells whether or not to include it in the outbound request. Elixir&#x27;s pattern matching syntax makes it a breeze to match the tables we want to send to Elasticsearch.</p><p>In this example, we will only put rows from <code>my_special_table</code> into the index.</p><h3>Request Builder</h3><p>In our webhook request, we return a closure that builds our WebhookRequest. The WebhookRequest allows you to set the method, headers and payload of your HTTP request as well as control what fields will be included in the payload. If you want to grab an environment variable, do it outside of the closure.</p><p>In Elasticsearch, bulk data is sent as newline-delimited JSON so we will have to format our paylaod accordingly.</p><h2>Configure Statetrace in Docker-Compose</h2><p>Now we will add Statetrace to our Docker-compose settings</p><h3>Get your License</h3><p>Go to <a href="https://www.statetrace.com/statetrace-core">https://www.statetrace.com/statetrace-core</a> to get your free <code>STATETRACE_LICENSE</code> key. No email or sign-up required.</p><h3>Make a secret</h3><p>Run the following command to generate a STATETRACE_SECRET_KEY </p><pre><code class="language-bash">head -c 500 /dev/urandom | tr -dc &#x27;a-zA-Z0-9&#x27; | fold -w 32 | head -n 1
</code></pre><h3>Add to YAML</h3><p>Taking our Statetrace License, our STATETRACE_INBOUND_CONFIG above and tie it all together with the rest of our docker compose file.</p><pre><code class="language-yaml" metastring="title=&quot;docker-compose.yaml&quot;" title="&quot;docker-compose.yaml&quot;">version: &#x27;3&#x27;
    
services:
  postgres:
    image: postgres
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

    command:
      - &quot;postgres&quot;
      - &quot;-c&quot;
      - &quot;wal_level=logical&quot;
      - &quot;-c&quot;
      - &quot;max_wal_senders=2&quot;
      - &quot;-c&quot;
      - &quot;max_replication_slots=2&quot;
    ports:
      - &quot;5432:5432&quot;

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.2
    container_name: es01
    environment:
      - node.name=es01
      - bootstrap.memory_lock=true
      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - elastic
  
  # The buffer database
  statetrace_db:
    image: postgres
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

    command:
      - &quot;postgres&quot;
      - &quot;-p&quot;
      - &quot;5433&quot;
    ports:
      - &quot;5433:5433&quot;


  statetrace:
    image: statetraceofficial/statetrace-beta
    environment:
      STATETRACE_DATABASE_URL: postgres://postgres:postgres@statetrace_db:5433/postgres
      STATETRACE_SECRET_KEY_BASE: &quot;123456789123456789123456789123456789123456789123456789123456789123456789&quot;
      STATETRACE_LICENSE: &quot;&lt;statetrace_license&gt;&quot;
      STATETRACE_INBOUND_CONFIG: |
          inbounds:
            - name: Postgres DB
              database_type: postgres
              username: postgres
              password: postgres
              database: postgres
              host: localhost
              publication: &quot;statetrace&quot;
              slot: &quot;statetrace&quot;
              port: 5432
              log_new_attributes: true
              outbounds:
                  - name: Webhook
                    handler: webhook
                    target_url: &quot;http://es01:9200/logs-statetrace_dump/_bulk&quot;
                    middleware: |
                      fn 
                        %{relation_name: tn} when tn in [&quot;my_special_table&quot;] -&gt; :ok
                        _ -&gt; :skip
                      end
                    request_builder: |
                        fn rows -&gt;
                          payload = 
                            rows
                            |&gt; Enum.flat_map(fn row -&gt; 
                                [
                                    Jason.encode_to_iodata!(%{&quot;create&quot; =&gt; %{}}),
                                    &quot;\n&quot;,
                                    Jason.encode_to_iodata!(Map.merge(%{&quot;@timestamp&quot; =&gt; row.row_timestamp}, row)),
                                    &quot;\n&quot;
                                ]
                            end)
                          %WebhookRequest{method: &quot;POST&quot;, headers: [{&quot;content-type&quot;, &quot;application/json&quot;}], payload: payload}
                        end
    depends_on:
      - statetrace_db
      - postgres
      - es01
</code></pre><p>Now all changes happening in the postgres database that match the middleware will be streamed into Elasticsearch under the <code>logs-statetrace_dump</code> index.</p><h2>Conclusion</h2><p>Keeping Postgres synchronized with Elasticsearch doesn&#x27;t need to be hard. We have shown that we can configure the standard webhooks found in Statetrace and use them to push data into the search engine without a lot of configuration.</p><p>You can expand on this method to customize the data that you choose to index.</p>]]></content>
        <author>
            <name>Kyle Hanson</name>
            <uri>https://github.com/hansonkd</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build an Elixir Redis API that's 100x faster than HTTP]]></title>
        <id>redis-server</id>
        <link href="https://docs.statetrace.com/blog/redis-server"/>
        <updated>2021-11-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Build a redis server in Elixir]]></summary>
        <content type="html"><![CDATA[<p>Need a fast server and client? HTTP too slow? Try the Redis Protocol for lightning fast, low-overhead API calls. It&#x27;s easy to implement and nearly every language has mature Redis clients that can connect.</p><div style="background:url(/img/ipanema.jpg);background-size:150px;width:100%;height:200px;margin-top:30px;margin-bottom:30px"></div><p><em>This project was inspired by <a href="https://github.com/hansonkd/Tino/">Tino</a>, the Redis/MsgPack framework for Python.</em></p><p>Building a server based on Redis Protocol from scratch can sound intimidating. But if you know what you are doing it can be relatively straight forward to implement. A huge shortcut involves using <a href="https://github.com/whatyouhide/redix">Redix</a> to parse the binary stream in a fast and efficient manner.</p><p>In this article we will implement a Redis echo server and explain how to extend the server to handle your own custom commands. In the end, it results in a performance boost of over <strong>100x</strong> using Redis instead of HTTP.</p><p>This blog post assumes you are familar with Elixir and its Application structure. If you want to learn more about TCP connections and supervision, read the <a href="https://elixir-lang.org/getting-started/mix-otp/task-and-gen-tcp.html">official Elixir article</a>.</p><h2>Advantages</h2><p>First before we get started, lets discuss some of the advantages. Building a server based on RESP (the Redis protocol) means you are cutting out a lot of overhead associated with HTTP. In addtion to a more lean protocol, nearly every language has a high-performance client built for Redis that allows pipelining. Pipelining combines commands as you send them for even greater efficiency. Most redis clients even support pooling for working at high concurrencies.</p><p>With these built in features, you don&#x27;t have to do very much to talk to your server in an extremely high performance fashion. </p><h2>Reading the TCP connection</h2><p>To get started building our server, we will need to accept a TCP connection. We do this by looping over <code>:gen_tcp.accept</code> and spawning a task.</p><pre><code class="language-elixir">defmodule MyRedisServer.Redis do
  require Logger
  def accept(port) do
    {:ok, socket} = :gen_tcp.listen(port, [:binary, active: false, reuseaddr: true])
    Logger.info(&quot;Accepting connections on port #{port}&quot;)
    loop_acceptor(socket)
  end

  defp loop_acceptor(socket) do
    {:ok, client} = :gen_tcp.accept(socket)

    {:ok, pid} =
      Task.start(fn -&gt;
        serve(client, %{continuation: nil})
      end)

    :ok = :gen_tcp.controlling_process(client, pid)

    loop_acceptor(socket)
  end
end
</code></pre><p>Now we are ready to read packets from the connection. Elixir&#x27;s Redis client Redix includes an parser for us to use.</p><pre><code class="language-elixir">defmodule MyRedisServer.Redis do
  ...

  defp serve(socket, %{continuation: nil}) do
    case :gen_tcp.recv(socket, 0) do
      {:ok, data} -&gt;  handle_parse(socket, Redix.Protocol.parse(data))
      {:error, :closed} -&gt; :ok
    end
  end

  defp serve(socket, %{continuation: fun}) do
    case :gen_tcp.recv(socket, 0) do
      {:ok, data} -&gt;  handle_parse(socket, fun.(data))
      {:error, :closed} -&gt; :ok
    end
  end
emd
</code></pre><p>Handling the parse result is straight forward. Either an entire message was processed and we can handle it, and respond, or a partial message was recieved and we need to wait for more data.</p><pre><code class="language-elixir">defmodule MyRedisServer.Redis do
  ...

  defp handle_parse(socket, {:continuation, fun}) do
    serve(socket, %{continuation: fun})
  end

  defp handle_parse(socket, {:ok, req, left_over}) do
    resp = handle(req)

    :gen_tcp.send(socket, Redix.Protocol.pack(resp))

    case left_over do
      &quot;&quot; -&gt; serve(socket, %{continuation: nil})
      _ -&gt; handle_parse(socket, Redix.Protocol.parse(left_over))
    end
  end

  def handle(data) do
    data
  end
end
</code></pre><h2>Complete example</h2><p>Finally we are ready to put it all together. All the pieces come together to form a nice little echo server.</p><pre><code class="language-elixir">defmodule MyRedisServer.Redis do
  require Logger

  def accept(port) do
    {:ok, socket} = :gen_tcp.listen(port, [:binary, active: false, reuseaddr: true])
    Logger.info(&quot;Accepting connections on port #{port}&quot;)
    loop_acceptor(socket)
  end

  defp loop_acceptor(socket) do
    {:ok, client} = :gen_tcp.accept(socket)

    {:ok, pid} =
      Task.start(fn -&gt;
        serve(client, %{continuation: nil})
      end)

    :ok = :gen_tcp.controlling_process(client, pid)

    loop_acceptor(socket)
  end

  defp serve(socket, %{continuation: nil}) do
    case :gen_tcp.recv(socket, 0) do
      {:ok, data} -&gt;  handle_parse(socket, Redix.Protocol.parse(data))
      {:error, :closed} -&gt; :ok
    end
  end

  defp serve(socket, %{continuation: fun}) do
    case :gen_tcp.recv(socket, 0) do
      {:ok, data} -&gt;  handle_parse(socket, fun.(data))
      {:error, :closed} -&gt; :ok
    end
  end

  defp handle_parse(socket, {:continuation, fun}) do
    serve(socket, %{continuation: fun})
  end

  defp handle_parse(socket, {:ok, req, left_over}) do
    resp = handle(req)

    :gen_tcp.send(socket, Redix.Protocol.pack(resp))

    case left_over do
      &quot;&quot; -&gt; serve(socket, %{continuation: nil})
      _ -&gt; handle_parse(socket, Redix.Protocol.parse(left_over))
    end
  end

  def handle(data) do
    data
  end
end

</code></pre><p>Run this server in your Application&#x27;s supervision tree:</p><pre><code class="language-elixir"> defmodule MyRedisServer.Application do
  use Application

  ...

  def start(_type, _args) do
    claims = get_license_claims!()

    children = [
      ...,
      Supervisor.child_spec({Task, fn -&gt; MyRedisServer.Redis.accept(3211) end},   restart: :permanent)
    ]

    ...

    Supervisor.start_link(children, opts)
  end
end
</code></pre><h2>Connecting from a client</h2><p>Start your mix project and you should be able to connect to redis on 3211 and the command should echo what you send it.</p><pre><code class="language-elixir">&gt; {:ok, conn} = Redix.start_link(&quot;redis://localhost:3211&quot;)
&gt; Redix.command(conn, [&quot;COOL_COMMAND&quot;, &quot;123&quot;])
{:ok, [&quot;COOL_COMMAND&quot;, &quot;123&quot;]}
</code></pre><p>Adding commands to your new redis server is easy with pattern matching:</p><pre><code class="language-elixir">defmodule MyRedisServer.Redis do
  ...

  def handle([&quot;PUT&quot;, key, val]) do
    Cachex.put(:my_cachex, key, val)
    [&quot;OK&quot;]
  end

  def handle([&quot;GET&quot;, key]) do
    [Cachex.get(:my_cachex, key)]
  end

  def handle([&quot;ECHO&quot;, msg]) do
    msg
  end

  def handle(_data) do
    %Redix.Error{message: &quot;UNKNOWN_COMMAND&quot;}
  end
end
</code></pre><h2>MsgPack</h2><p>MsgPack is essentially a faster, more compact version of JSON. Use it to serialize complex structures into binary data to pass back and forth between your API.</p><pre><code class="language-elixir">defmodule MyRedisServer.Redis do
  ...

  def handle([command, payload]) do
    case handle_command(command, MsgPax.unpack!(payload)) do
        {:error, e} -&gt; %Redix.Error{message: &quot;ERROR #{e}&quot;}
        value -&gt; [MsgPax.pack!(value)]
    end
  end

  def hande(_) do
    %Redix.Error{message: &quot;INMVALID_FORMAT&quot;}
  end

  defp handle_command(&quot;PUT&quot;, [key, val]) do
    Cachex.put(:my_cachex, key, val)
    [&quot;OK&quot;]
  end

  defp handle_command(&quot;GET&quot;, key) do
    Cachex.get(:my_cachex, key)
  end

  defp handle_command(&quot;ECHO&quot;, msg) do
    msg
  end

  defp handle_command(_command, _data) do
    {:error, &quot;INVALID_COMMAND&quot;}
  end
end
</code></pre><h2>Benchmark</h2><p>For this benchmark we will compare HTTP Phoenix to our Redis Server.</p><p>Our HTTP Phoenix Controllers:</p><pre><code class="language-elixir">  # GET -&gt; Text
  def bench(conn, %{&quot;payload&quot; =&gt; payload, &quot;times&quot; =&gt; times}) when is_binary(times) do
    text(conn, String.duplicate(payload, String.to_integer(times)))
  end

  # POST -&gt; JSON
  def bench(conn, %{&quot;payload&quot; =&gt; payload, &quot;times&quot; =&gt; times}) do
    json(conn, %{&quot;data&quot; =&gt; String.duplicate(payload, times)})
  end
</code></pre><p>and our Redis server:</p><pre><code class="language-elixir">  def handle([&quot;BENCH&quot;, payload, number]) do
    [String.duplicate(payload, String.to_integer(number))]
  end
</code></pre><p>We will use <a href="https://github.com/sneako/finch">Finch</a> for the HTTP client, which labels itself as &quot;performance focused&quot;.</p><p>For the full benchmark see <a href="https://gist.github.com/hansonkd/cd34329fe4f346e680b39a17d9988af4">the source</a>.</p><p>We will remotely call our functions using the Finch HTTP pool, a single Redix connection, or a pool of Redix connections. We will also test pipelining vs calling each command individually for Redix. We will call our remote function 1000 times concurrently and ask it to duplicate the string <code>&quot;12345&amp;?\&quot;678,\n90&quot;</code>  100 times and respond.</p><pre><code class="language-bash">Name                           ips        average  deviation         median         99th %
redix_pool                   70.44       14.20 ms    ±36.07%       13.30 ms       50.60 ms
run_redix_pipeline           30.56       32.73 ms    ±65.74%       47.26 ms       91.99 ms
redix_pool_pipelined         21.55       46.40 ms     ±3.87%       47.59 ms       48.12 ms
redix                        13.84       72.28 ms     ±9.91%       72.09 ms       80.31 ms
finch_get                     0.55     1814.88 ms     ±2.44%     1814.88 ms     1846.24 ms
finch_post                    0.54     1859.71 ms     ±0.70%     1859.71 ms     1868.97 ms
</code></pre><p>The results show that running Redis protocol is well over 100x faster than relying on HTTP. By default Phoenix sends extra headers for the content type and other information. In addition there is extra overhead encoding and decoding the values for URL encoding and JSON.</p><p>Overall using Redis as a Protocol instead of HTTP results in orders of magnitude higher troughput.</p><h2>Conclusion</h2><p>We wrote a high-performance server based on the Redis Protocol in around 10 minutes. This server can handle thousands of connections easily and has minimal overhead. One downside is that load balancing becomes more of a challenge when doing multi-node deploys when using a protocol other than HTTP.</p><p>If you have a one or thousands of clients that need to communicate with a server in the fastest way possible, consider using Redis as your protocol of choice instead of HTTP.</p>]]></content>
        <author>
            <name>Kyle Hanson</name>
            <uri>https://github.com/hansonkd</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time for a Change - Announcing Statetrace]]></title>
        <id>time-for-a-change</id>
        <link href="https://docs.statetrace.com/blog/time-for-a-change"/>
        <updated>2021-11-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Data auditing with Statetrace]]></summary>
        <content type="html"><![CDATA[<p>The world of application data today is broken. We treat our most valuable asset as a second-class citizen, only keeping the most recent version data available and throwing away critical information of how the data got there. It&#x27;s is a hard problem to fix. Statetrace is here to make it easier.</p><div style="background:url(/img/ipanema.jpg);background-size:150px;width:100%;height:200px;margin-top:30px;margin-bottom:30px"></div><p>Data auditing is the future of application development. It enables teams to solve tougher problems faster. With Statetrace, teams can develop an auditing solution in hours instead of months and start delivering reliable answers to their customers. Statetrace annotates row level changes from your databases, piping them into webhooks, into data warehouses, or indexing them for fast searches.</p><h2>Whats wrong today</h2><p>The world of auditing today has been stuck for decades. There exist <a href="https://django-simple-history.readthedocs.io/en/latest/">libraries</a> and <a href="https://github.com/etianen/django-reversion">tools</a> at the application layer to associate changes with who changed them, however these tools are deeply flawed. Because they work at the application layer, they miss things that don&#x27;t go through the application; like migrations or someone connected directly to the DB. More importantly they are strictly framework dependent and do not offer a general solution.</p><p>CDC pipelines capture the changes accurately but do not associate those changes with application meta information.</p><h2>How it works</h2><p>Statetrace connects to the <a href="https://www.postgresql.org/docs/10/logical-replication.html">logical replication</a> of Postgres or the <a href="https://dev.mysql.com/doc/internals/en/binary-log-overview.html">BinLog</a> of MySQL. By reading events directly from the replication log, Statetrace gets a 100% accurate history of your data. More importantly because it is intefacing with the database instead of the application, Statetrace can work with any framework or language with minimal configuration.</p><p>The application annotates transactions by writing to an annotations table in the same transaction that you change other data, associating session and user information with individual row changes.</p><h2>Solving the &quot;Who dunnit?&quot;</h2><p>Nobody wants to answer the dreaded customer complaint &quot;Who changed my data?&quot; Even if the customer was the last one to change the data, without a proper auditing solution one might not be able to give a reliable answer. Having uncertainty around the history of data puts a company&#x27;s reputation at risk.</p><p>Statetrace puts the answers to these questions at your fingertips. With an annotated audit log, each row change is associated with meta-information about who in the application changed the data. Pipe these changes into the destination of your choice for easy searching.</p><h2>Github for Code. Statetrace for Data.</h2><p>Version control for code is integral to the development process. Companies spend billions of dollars every year on developer salaries and want to keep that investment of developer output. Data about what the code was and who changed it is so valuable that a multi-billion dollar industry has grown to support those needs. </p><p>However, companies are throwing away money when it comes to their actual bread and butter: the application data. The stream of changes from application data are a gold mine for solving problems and answering questions. But the vast majority of companies today throw away these changes, because they have little value as they don&#x27;t connect the change to who made the change. This is bad, because you don&#x27;t know what type of questions you might want to know in the future and once you throw it away, its gone for ever.</p><p>Statetrace is solving this problem. Statetrace makes the stream of changes useful by associating the change with who changed it and simplifying piping these changes into other data sources.</p><h2>Time traveling SQL</h2><p>Once all changes from a DB are collected, they can be used to recreate transaction-level point-in-time snapshots of your entire database, a particular table, or just a single row. This allows you to easily go back to see what a the result of a query was. It also helps you answer more interesting questions in data analytics as you can compare two points in time in a single query all in your existing data warehouse.</p><h2>Compliance focused</h2><p>Your data belongs with you. Statetrace is designed to be run on-prem, leaving you in complete control of your data. Whether you are running a HIPAA deployment or need to stay SOC 2 compliant, Statetrace works with your compliance team to succeed.</p><h2>Statetrace Core</h2><p>Users can try <a href="/docs/intro">Statetrace Core</a> today for free. Its a limited edition of statetrace without a UI, but with all of the power. Try it out locally to quickly connect your database and start scanning within minutes.</p><h2>Statetrace Enterprise</h2><p>Our flagship product is <a href="https://www.statetrace.com">Statetrace for Enterprise</a>. Its the full featured Statetrace experience with a robust UI, enterprise level user permissions, pre-constructed SQL models for time-travel and support from our customer success team.</p><h2>The future</h2><p>We are developing the highest quality auditing experience. If what we are doing sounds interesting, reach out at <a href="mailto:hello@statetrace.com">hello@statetrace.com</a> and we would love to tell you about what we are working on.</p>]]></content>
        <author>
            <name>Kyle Hanson</name>
            <uri>https://github.com/hansonkd</uri>
        </author>
    </entry>
</feed>